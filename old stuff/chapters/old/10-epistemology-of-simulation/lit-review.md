# Epistemology


Hegselmann, R. and U. Krause. 2006. “Truth and Cognitive Division of Labour: First Steps Towards a Computer-Aided Social Epistemology.” Journal of Artificial Societies and Social Simulation 9(3).

Zollman, K. J. 2007. “The Communication Structure of Epistemic Communities.”
Philosophy of Science 74(5): 574–87.


Simulation in Science

Winsberg, Eric B. (2010). Science in the Age of Computer Simulation. University of Chicago Press.
Helgeson, Casey ; Srikrishnan, Vivek ; Keller, Klaus & Tuana, Nancy, Why Simpler Computer Simulation Models can be Epistemically Better for Informing Decisions.
Parker, Wendy (2009). Does matter really matter? Computer simulations, experiments, and materiality. Synthese 169 (3):483-496.
Humphreys, Paul (2004). Extending Ourselves: Computational Science, Empiricism, and Scientific Method. Oxford University Press.
Aronowitz, Sara & Lombrozo, Tania (forthcoming). Learning through Simulation. Philosophers' Imprint.
Frigg, Roman & Reiss, Julian (2009). The philosophy of simulation: hot new issues or same old stew? Synthese 169 (3):593-613.
Stuart, Michael T. & Nersessian, Nancy J. (2018). Peeking Inside the Black Box: A New Kind of Scientific Visualization. Minds and Machines 29 (1):87-107.
Bokulich, Alisa (forthcoming). Towards a Taxonomy of the Model-Ladenness of Data. PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association.
Franceschelli, Sara (2009). Computer Simulations as Experiments. Synthese 169 (3):557 - 574.
Stuart, Michael T. (2017). Imagination: A Sine Qua Non of Science. Croatian Journal of Philosophy (49):9-32.
Dardashti, Radin ; Y. Thébault, Karim P. & Winsberg, Eric (2017). Confirmation via Analogue Simulation: What Dumb Holes Could Tell Us about Gravity. British Journal for the Philosophy of Science 68 (1).
Arnold, Eckhart (2019). Validation of Computer Simulations from a Kuhnian Perspective. In Claus Beisbart & Nicole J. Saam (eds.), Computer Simulation Validation - Fundamental Concepts, Methodological Frameworks, and Philosophical Perspectives. Heidelberg, Deutschland: Springer. pp. 203-224.
Greene, Preston (forthcoming). The Termination Risks of Simulation Science. Erkenntnis:1-21.
Rathkopf, Charles (2018). Network representation and complex systems. Synthese (1).
Frigg, Roman & Reiss, Julian (2009). A Critical Look at the Philosophy of Simulation. Synthese 169 (3).
Parker, Wendy S. (2017). Computer Simulation, Measurement, and Data Assimilation. British Journal for the Philosophy of Science 68 (1):273-304.
Durán, Juan Manuel (2018). Computer Simulations in Science and Engineering. Concept, Practices, Perspectives. Springer.
Weber, Marcel (2014). Experimental Modeling in Biology: In Vivo Representation and Stand-ins As Modeling Strategies. Philosophy of Science 81 (5):756-769.
Varenne, Franck (2017). Théories et modèles en sciences humaines. Le cas de la géographie. Paris, France: Editions Matériologiques.
Varenne, Franck (2018). From Models to Simulations. London, UK: Routledge.
Rohrlich, Fritz (1990). Computer Simulation in the Physical Sciences. PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association 1990:507-518.
Varenne, Franck & Phan, Denis (2008). Agent-Based Models and Simulations in Economics and Social Sciences: from conceptual exploration to distinct ways of experimenting. In Nuno David, José Castro Caldas & Helder Coelho (eds.), Proceedings of the 3rd EPOS congress (Epistemological Perspectives On Simulations). Lisbon: pp. 51-69.
Varenne, Franck (2001). What does a Computer Simulation prove? The case of plant modeling at CIRAD. In N. Giambiasi & C. Frydman (eds.), Simulation in industry - ESS 2001, Proc. of the 13th European Simulation Symposium. Society for Computer Simulation (SCS).
Varenne, Franck (2013). Chains of Reference in Computer Simulations. FMSH Working Papers 51:1-32.
Gebharter, Alexander & Koch, Daniel (2018). Combining causal Bayes nets and cellular automata: A hybrid modelling approach to mechanisms. British Journal for the Philosophy of Science.
Varenne, Franck (2003). La simulation conçue comme expérience concrète. In Jean-Pierre Müller (ed.), Le statut épistémologique de la simulation. Editions de l'ENST.
Miłkowski, Marcin (2013). Reverse-engineering in Cognitive-Science. In Marcin Miłkowski & Konrad Talmont-Kaminski (eds.), Regarding Mind, Naturally. Cambridge Scholars Press. pp. 12-29.
Zednik, Carlos (2015). Heuristics, Descriptions, and the Scope of Mechanistic Explanation. In P. Braillard & C. Malaterre (eds.), Explanation in Biology. An Enquiry into the Diversity of Explanatory Patterns in the Life Sciences. Dordrecht: Springer. pp. 295-318.
Epstein, Brian (2011). Agent-based modeling and the fallacies of individualism. In Paul Humphreys & Cyrille Imbert (eds.), Models, Simulations, and Representations. Routledge. pp. 115444.
Tal, Eran (2011). From data to phenomena and back again: computer-simulated signatures. Synthese 182 (1):117-129.
Humphreys, Paul (1995). Computational science and scientific method. Minds and Machines 5 (4):499-512.
Grim, Patrick ; Rosenberger, Robert ; Rosenfeld, Adam ; Anderson, Brian & Eason, Robb E. (2013). How simulations fail. Synthese 190 (12):2367-2390.
Varenne, Franck (2013). The Nature of Computational Things. In Frédéric Migayrou Brayer & Marie-Ange (eds.), Naturalizing Architecture. Orléans: HYX Editions. pp. 96-105.
Parker, Matthew W. (2009). Computing the uncomputable; or, The discrete charm of second-order simulacra. Synthese 169 (3):447-463.
Varenne, Franck (2010). Framework for Models and Simulations with Agents in regard to Agent Simulations in Social Sciences: Emulation and Simulation. In Alexandre Muzy, David R. C. Hill & Bernard P. Zeigler (eds.), Activity-Based Modeling and Simulation. Presses Universitaires Blaise-Pascal.
Carusi, Annamaria (2012). Making the Visual Visible in Philosophy of Science. Spontaneous Generations 6 (1):106-114.
Durán, Juan M. (2013). A brief overview of the philosophical study of computer simulations. American Philosophical Association Newsletter on Philosophy and Computers 13 (1):38-46.
Miłkowski, Marcin (2015). Evaluating Artificial Models of Cognition. Studies in Logic, Grammar and Rhetoric 40 (1):43-62.
Varenne, Franck (2007). Du Modèle à la Simulation Informatique. Vrin.
Varenne, Franck (2009). Simulation informatique et pluriformalisation des objets composites. Philosophia Scientiae 13 (1):135-154.
Jacquart, Melissa (2018). Learning About Reality Through Models and Computer Simulations. Science & Education 27 (7-8):805-810.
Varenne, Franck (2006). Les notions de métaphore et d'analogie dans les épistémologies des modèles et des simulations. Paris, France: Editions Petra.
Muzy, Alexandre ; Varenne, Franck ; Zeigler, Bernard P. ; Caux, Jonathan ; Coquillard, Patrick ; Touraille, Luc ; Prunetti, Dominique ; Caillou, Philippe ; Michel, Olivier & Hill, David R. C. (2013). Refounding of the activity concept? Towards a federative paradigm for modeling and simulation. Simulation - Transactions of the Society for Modeling and Simulation International 89 (2):156-177.
Varenne, Franck (2009). Models and Simulations in the Historical Emergence of the Science of Complexity. In Ma Aziz-Alaoui & C. Bertelle (eds.), From System Complexity to Emergent Properties. Springer. pp. 3--21.
Laymon, Ronald (1990). Computer Simulations, Idealizations and Approximations. PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association 1990:519 - 534.
Malapi-Nelson, Alcibiades (2014). Humanities’ metaphysical underpinnings of late frontier scientific research. Humanities 214 (3):740-765.
Varenne, Franck (2003). La simulation informatique face à la « méthode des modèles ». Le cas de la croissance des plantes. Natures Sciences Sociétés 11 (1):16-28.
Arnold, Eckhart, The dark side of the force: When computer simulations lead us astray and 'model think' narrows our imagination.
Sawyer, R. Keith (2004). Social explanation and computational simulation. Philosophical Explorations 7 (3):219-231.
Varenne, Franck (2008). Émergences par les règles sans « formes de vie » une relecture de Kripke (1982) pour la simulation informatique du vivant. Noesis 14:201-236.
Hillerbrand, Rafaela (2012). Order out of Chaos? A Case Study in High Energy Physics. Studia Philosophica Estonica 5 (2):61-78.
Zednik, Carlos (2018). Models and mechanisms in network neuroscience. Philosophical Psychology 32 (1):23-51.
Varenne, Franck (2011). Modéliser le social. Méthodes fondatrices et évolutions récentes. Paris, France: Dunod.
Ruyant, Quentin (2018). Compte rendu de L’observation scientifique, aspects philosophiques et pratiques de Vincent Israel-Jost. Lato Sensu, Revue de la Société de Philosophie des Sciences 5:41-43.
Wenmackers, Sylvia & Vanpoucke, Danny E. P. (2012). Models and simulations in material science: two cases without error bars. Statistica Neerlandica 66 (3):339–355.
Varenne, Franck (2012). Quelques aspects de l’œuvre de Jean-Marie Legay. Natures Sciences Sociétés 20 (4):461-463.
Colombo, Matteo (2015). Why Build a Virtual Brain? Large-scale Neural Simulations as Test-bed for Artificial Computing Systems. In D. C. Noelle, R. Dale, A. S. Warlaumont, J. Yoshimi, T. Matlock, C. D. Jennings & P. P. Maglio (eds.), Proceedings of the 37th Annual Conference of the Cognitive Science Society. Cognitive Science Society. pp. 429-434.
Varenne, Franck (2009). Simulation informatique et pluriformalisation des objets composites. Philosophia Scientae 13:135-154.
D'Souza, Raissa M. (2006). BML revisited: Statistical physics, computer simulation, and probability. Complexity 12 (2):30-39.
Gelfert, Axel (2011). Scientific models, simulation, and the experimenter's regress. In Paul Humphreys & Cyrille Imbert (eds.), Models, Simulations, and Representations. Routledge.
Fernández, Jordi (2003). Explanation by computer simulation in cognitive science. Minds and Machines 13 (2):269-284.
Loula, Angelo ; Gudwin, Ricardo ; El-Hani, Charbel & Queiroz, João, The emergence of symbol-based communication in a complex system of artificial creatures.
Blomberg, Olle (2009). Review of Sherry Turkle’s ‘Simulation and Its Discontents’. Metapsychology Online Reviews 13 (47).
Varenne, Franck (2006). Bachelard avec la simulation informatique: nous faut-il reconduire sa critique de l'intuition ? In Robert Damien & B. Hufschmitt (eds.), Bachelard: Confiance Raisonnée Et Défiance Rationnelle. Besançon: Presses Universitaires de Franche-Comté. pp. 111-143.
Sargent, Pauline (1996). On the use of visualizations in the practice of science. Philosophy of Science 63 (3):238.
Beaulieu, Anne ; Ratto, Matt & Scharnhorst, Andrea (2013). Learning in a landscape: simulation-building as reflexive intervention. Mind and Society 12 (1):91-112.
Amigoni, Francesco & Schiaffonati, Viola (2007). Multiagent-based simulation in biology. In L. Magnani & P. Li (eds.), Model-Based Reasoning in Science, Technology, and Medicine. Springer. pp. 179--191.
Varenne, Franck (2012). Théorie, Réalité, Modèle. Paris, France: Editions Matériologiques.
Feltrero, Roberto (2005). The role of computers in scientific research: a cognitive approach. In L. Magnani & R. Dossena (eds.), Computing, Philosophy and Cognition. pp. 87--98.
Eisenberg, Michael (forthcoming). Combining qualitative and quantitative techniques in the simulation of chemical reaction mechanisms. Ai and Simulation: Theory and Applications (Simulation Series Vol. 22, No. 3.). Society for Computer Simulation, San Diego. Ca.
Newman, Julian (2016). Epistemic opacity, confirmation holism and technical debt: computer simulation in the light of empirical software engineering. In History and Philosophy of Computing (IFIP AICT 487). Cham, Switzerland: Springer. pp. 256-272.
Wieber, Frederic, Theoretical technologies in an “experimental” setting: Empirical modeling of proteinic objects and simulation of their dynamics within scientific collaborations around a supercomputer.
Saito, Ron Shiro (2000). From Instructional Social Computer Simulation to Heidegger's Aesthetics. Dissertation, Indiana University



@article{Angius2011-ANGSTO,
  pages = {323--336},
  number = {2},
  journal = {Minds and Machines},
  year = {2011},
  title = {Scientific Theories of Computational Systems in Model Checking},
  volume = {21},
  author = {Nicola Angius and Guglielmo Tamburrini}
}

> Model checking, a prominent formal method used to predict and explain the behaviour of software and hardware systems, is examined on the basis of reflective work in the philosophy of science concerning the ontology of scientific theories and model-based reasoning. The empirical theories of computational systems that model checking techniques enable one to build are identified, in the light of the semantic conception of scientific theories, with families of models that are interconnected by simulation relations. And the mappings between these scientific theories and computational systems in their scope are analyzed in terms of suitable specializations of the notions of model of experiment and model of data. Furthermore, the extensively mechanized character of model-based reasoning in model checking is highlighted by a comparison with proof procedures adopted by other formal methods in computer science. Finally, potential epistemic benefits flowing from the application of model checking in other areas of scientific inquiry are emphasized in the context of computer simulation studies of biological information processing 

@article{Angius2014-ANGTPO-10,
  year = {2014},
  number = {3},
  pages = {423--439},
  journal = {Philosophy and Technology},
  author = {Nicola Angius},
  volume = {27},
  title = {The Problem of Justification of Empirical Hypotheses in Software Testing}
}

> This paper takes part in the methodological debate concerning the nature and the justification of hypotheses about computational systems in software engineering by providing an epistemological analysis of Software Testing, the practice of observing the programs’ executions to examine whether they fulfil software requirements. Property specifications articulating such requirements are shown to involve falsifiable hypotheses about software systems that are evaluated by means of tests which are likely to falsify those hypotheses. Software Reliability metrics, used to measure the growth of probability that given failures will occur at specified times as new executions are observed, is shown to involve a Bayesian confirmation of falsifiable hypotheses on programs. Coverage criteria, used to select those input values with which the system under test is to be launched, are understood as theory-laden principles guiding software tests, here compared to scientific experiments. Redundant computations, fault seeding models and formal methods used in software engineering to evaluate test results are taken to be instantiations of some epistemological strategies used in scientific experiments to distinguish between valid and non-valid experimental outcomes. The final part of the paper explores the problem, advanced in the context of the philosophy of technology, of defining the epistemological status of software engineering by conceiving it as a scientifically attested technology 

@article{Primiero2015-PRIOMS,
  publisher = {Springer Netherlands},
  year = {2015},
  volume = {192},
  author = {Giuseppe Primiero and Nir Fresco and Luciano Floridi},
  number = {4},
  journal = {Synthese},
  pages = {1199--1220},
  title = {On Malfunctioning Software}
}

> Artefacts do not always do what they are supposed to, due to a variety of reasons, including manufacturing problems, poor maintenance, and normal wear-and-tear. Since software is an artefact, it should be subject to malfunctioning in the same sense in which other artefacts can malfunction. Yet, whether software is on a par with other artefacts when it comes to malfunctioning crucially depends on the abstraction used in the analysis. We distinguish between “negative” and “positive” notions of malfunction. A negative malfunction, or dysfunction, occurs when an artefact token either does not or cannot do what it is supposed to. A positive malfunction, or misfunction, occurs when an artefact token may do what is supposed to but, at least occasionally, it also yields some unintended and undesirable effects. We argue that software, understood as type, may misfunction in some limited sense, but cannot dysfunction. Accordingly, one should distinguish software from other technical artefacts, in view of their design that makes dysfunction impossible for the former, while possible for the latter. 


@article{Fillion2014-FILOTE,
  journal = {Synthese},
  number = {7},
  volume = {191},
  year = {2014},
  author = {Nicolas Fillion and Robert M. Corless},
  title = {On the Epistemological Analysis of Modeling and Computational Error in the Mathematical Sciences},
  pages = {1451--1467}
}

> Interest in the computational aspects of modeling has been steadily growing in philosophy of science. This paper aims to advance the discussion by articulating the way in which modeling and computational errors are related and by explaining the significance of error management strategies for the rational reconstruction of scientific practice. To this end, we first characterize the role and nature of modeling error in relation to a recipe for model construction known as Euler’s recipe. We then describe a general model that allows us to assess the quality of numerical solutions in terms of measures of computational errors that are completely interpretable in terms of modeling error. Finally, we emphasize that this type of error analysis involves forms of perturbation analysis that go beyond the basic model-theoretical and statistical/probabilistic tools typically used to characterize the scientific method; this demands that we revise and complement our reconstructive toolbox in a way that can affect our normative image of science. 


@article{Fetzer1988-FETPVT,
  journal = {Communications of the Acm},
  number = {9},
  pages = {1048--1063},
  year = {1988},
  author = {James H. Fetzer},
  title = {Program Verification: The Very Idea},
  volume = {31}
}

> The notion of program verification appears to trade upon an equivocation. Algorithms, as logical structures, are appropriate subjects for deductive verification. Programs, as causal models of those structures, are not. The success of program verification as a generally applicable and completely reliable method for guaranteeing program performance is not even a theoretical possibility. 


@article{Fetzer1991-FETPAO,
  pages = {197--216},
  number = {2},
  year = {1991},
  author = {James H. Fetzer},
  journal = {Minds and Machines},
  title = {Philosophical Aspects of Program Verification},
  volume = {1}
}

> A debate over the theoretical capabilities of formal methods in computer science has raged for more than two years now. The function of this paper is to summarize the key elements of this debate and to respond to important criticisms others have advanced by placing these issues within a broader context of philosophical considerations about the nature of hardware and of software and about the kinds of knowledge that we have the capacity to acquire concerning their performance. 


@article{David2009-DAVVAV-2,
  year = {2009},
  author = {Nuno David},
  pages = {117--129},
  journal = {Epistemological Aspects of Computer Simulation in the Social Sciences, EPOS 2006, Revised Selected and Invited Papers, Lecture Notes in Artificial Intelligence, Squazzoni, Flaminio (Ed.)},
  volume = {5466},
  title = {Validation and Verification in Social Simulation: Patterns and Clarification of Terminology}
}

> The terms ‘verification’ and ‘validation’ are widely used in science, both in the natural and the social sciences. They are extensively used in simulation, often associated with the need to evaluate models in different stages of the simulation development process. Frequently, terminological ambiguities arise when researchers conflate, along the simulation development process, the technical meanings of both terms with other meanings found in the philosophy of science and the social sciences. This article considers the problem of verification and validation in social science simulation along five perspectives: The reasons to address terminological issues in simulation; the meaning of the terms in the philosophical sense of the problem of “truth”; the observation that some debates about these terms in simulation are inadvertently more terminological than epistemological; the meaning of the terms in the technical context of the simulation development process; and finally, a comprehensive outline of the relation between terminology used in simulation, different types of models used in the development process and different epistemological perspectives. 



@article{Angius2013-ANGAAI,
  pages = {211--226},
  journal = {Minds and Machines},
  title = {Abstraction and Idealization in the Formal Verification of Software Systems},
  number = {2},
  author = {Nicola Angius},
  year = {2013},
  volume = {23}
}

> Questions concerning the epistemological status of computer science are, in this paper, answered from the point of view of the formal verification framework. State space reduction techniques adopted to simplify computational models in model checking are analysed in terms of Aristotelian abstractions and Galilean idealizations characterizing the inquiry of empirical systems. Methodological considerations drawn here are employed to argue in favour of the scientific understanding of computer science as a discipline. Specifically, reduced models gained by Dataion are acknowledged as Aristotelian abstractions that include only data which are sufficient to examine the interested executions. The present study highlights how the need to maximize incompatible properties is at the basis of both Abstraction Refinement, the process of generating a cascade of computational models to achieve a balance between simplicity and informativeness, and the Multiple Model Idealization approach in biology. Finally, fairness constraints, imposed to computational models to allow fair behaviours only, are defined as ceteris paribus conditions under which temporal formulas, formalizing software requirements, acquire the status of law-like statements about the software systems executions 



@incollection{David2007-DAVSAF,
  title = {Simulation as Formal and Generative Social Science: The Very Idea},
  booktitle = {Worldviews, Science, and Us: Philosophy and Complexity},
  pages = {266--275},
  author = {Nuno David and Jaime Sichman and Helder Coelho},
  editor = {Carlos Gershenson and Diederik Aerts and Bruce Edmonds},
  year = {2007},
  publisher = {World Scientific}
}

> The formal and empirical-generative perspectives of computation are demonstrated to be inadequate to secure the goals of simulation in the social sciences. Simulation does not resemble formal demonstrations or generative mechanisms that deductively explain how certain models are sufficient to generate emergent macrostructures of interest. The description of scientific practice implies additional epistemic conceptions of scientific knowledge. Three kinds of knowledge that account for a comprehensive description of the discipline were identified: formal, empirical and intentional knowledge. The use of formal conceptions of computation for describing simulation is refuted; the roles of programming languages according to intentional accounts of computation are identified; and the roles of iconographic programming languages and aesthetic machines in simulation are characterized. The roles that simulation and intentional decision making may be able to play in a participative information society are also discussed. 


@article{Vakarelov2012-VAKTIM,
  title = {The Information Medium},
  pages = {47--65},
  author = {Orlin Vakarelov},
  volume = {25},
  year = {2012},
  journal = {Philosophy and Technology},
  number = {1}
}

> The paper offers the foundations of the theory of information media. Information media are dynamical systems with additional macrostructure of information-carrying states and information-preserving transformations. The paper also defines the notion of information media network as a system of information media connected by information transformations. It is demonstrated that many standard examples of information-containing and processing systems are captured by the general notion of information medium. The paper uses the theory (and informal discussion) of information media to motivate a structural approach to the information in media. The idea is that the notion of information transformation should be regarded as more primitive than the notion of informational state. Thus in information systems, especially in the context of information technology, information is secondary while information transformation is primary 

@article{Love2008-LOVAPO-2,
  pages = {27--30},
  number = {2},
  journal = {Philosophy of Management},
  title = {A Philosophy of Maintenance? Engaging with the Concept of Software},
  year = {2008},
  volume = {6},
  publisher = {Libri Publishing},
  author = {David Love}
}

> Although reducing the costs of software maintenance has long been held as an important goal, few researchers have studied software maintenance - except in the context of software design. However, thinking in software design is itself muddled by the frequent confusion over the term ‘software’ and ‘programs’. In this paper we argue for a re-examination of the underlying philosophical foundations of programs, in order to establish software as a phenomenon in its own right. Once we understand the basic structure of software theories, we will be in a better position to understand how theories of software relate to theories of programs. This might finally provide the insight needed to achieve the long awaited reduction in the cost of software maintenance 


@incollection{Dodig-Crnkovic2007-DODP-3,
  year = {2007},
  title = {WHERE DO NEW IDEAS COME FROM? HOW DO THEY EMERGE? - EPISTEMOLOGY AS COMPUTATION},
  editor = {Christian Calude},
  booktitle = {Randomness \& Complexity, from Leibniz to Chaitin},
  author = {Gordana Dodig{-}Crnkovic}
}

> This essay presents arguments for the claim that in the best of all possible worlds (Leibniz) there are sources of unpredictability and creativity for us humans, even given a pancomputational stance. A suggested answer to Chaitin’s questions: “Where do new mathematical and biological ideas come from? How do they emerge?” is that they come from the world and emerge from basic physical (computational) laws. For humans as a tiny subset of the universe, a part of the new ideas comes as the result of the re-configuration and reshaping of already existing elements and another part comes from the outside as a consequence of openness and interactivity of the system. For the universe at large it is randomness that is the source of unpredictability on the fundamental level. In order to be able to completely predict the Universe-computer we would need the Universe-computer itself to compute its next state; as Chaitin already demonstrated there are incompressible truths which means truths that cannot be computed by any other computer but the universe itself. 


@article{Angius2017-ANGEEC,
  pages = {239--258},
  publisher = {Springer Verlag},
  volume = {30},
  year = {2017},
  doi = {10.1007/s13347-016-0235-1},
  number = {2},
  journal = {Philosophy and Technology},
  title = {Explaining Engineered Computing Systems\textquoteright Behaviour: The Role of Abstraction and Idealization},
  author = {Nicola Angius and Guglielmo Tamburrini}
}

@incollection{Duran2013-DURTUO-4,
  booktitle = {Computer simulations and the changing face of scientific experimentation},
  editor = {Juan M. Dur\'an and Eckhart Arnold},
  title = {The Use of the \textquoteleftMateriality Argument\textquoteright in the Literature on Computer Simulations},
  author = {Juan M. Dur\'an},
  pages = {76--98},
  year = {2013}
}


@article{Sergeyev2013-SERSAM,
  pages = {645--663},
  volume = {65},
  year = {2013},
  number = {2},
  journal = {Journal of Supercomputing},
  author = {Yaroslav Sergeyev and Alfredo Garro},
  title = {Single-Tape and Multi-Tape Turing Machines Through the Lens of the Grossone Methodology}
}

@article{Angius2011-ANGSTO,
  number = {2},
  volume = {21},
  journal = {Minds and Machines},
  year = {2011},
  author = {Nicola Angius and Guglielmo Tamburrini},
  title = {Scientific Theories of Computational Systems in Model Checking},
  doi = {10.1007/s11023-011-9231-5},
  pages = {323--336}
}

@incollection{Wheeler2016-WHEMEA,
  booktitle = {Routledge Companion to Philosophy of Social Science},
  publisher = {Routledge},
  year = {2016},
  author = {Gregory Wheeler},
  title = {Machine Epistemology and Big Data},
  editor = {Lee McIntyre and Alex Rosenburg}
}

> In the age of big data and a machine epistemology that can anticipate, predict, and intervene on events in our lives, the problem once again is that a few individuals possess the knowledge of how to regulate these activities. But the question we face now is not how to share such knowledge more widely, but rather of how to enjoy the public beneﬁts bestowed by this knowledge without freely sharing it. It is not merely personal privacy that is at stake but a range of unsung beneﬁts that come from ignorance and forgetting, traits that are inherently human and integral to the functioning of our society. 

@article{Brey2005-BRETEA,
  volume = {15},
  number = {3-4},
  author = {Philip Brey},
  year = {2005},
  journal = {Minds and Machines},
  doi = {10.1007/s11023-005-9003-1},
  title = {The Epistemology and Ontology of Human-Computer Interaction},
  pages = {383--398},
  publisher = {Springer}
}

@article{Wheeler2013-WHEMMA,
  author = {Gregory Wheeler},
  year = {2013},
  journal = {Metaphilosophy},
  volume = {44},
  number = {3},
  pages = {293--300},
  publisher = {Wiley-Blackwell},
  doi = {10.1111/meta.12036},
  title = {Models, Models, and Models}
}



@incollection{VanDenBerg2018-VANAPP-11,
  year = {2018},
  booktitle = {3rd Workshop on Visualization for the Digital Humanities},
  author = {Hein Van Den Berg and Arianna Betti and Thom Castermans and Rob Koopman and Bettina Speckmann and K. A. B. Verbeek and Titia Van der Werf and Shenghui Wang and Michel A. Westenberg},
  title = {A Philosophical Perspective on Visualization for Digital Humanities},
  editor = {}
}

> In this position paper, we describe a number of methodological and philosophical challenges that arose within our interdisciplinary Digi- tal Humanities project CATVIS, which is a collaboration between applied geometric algorithms and visualization researchers, data scientists working at OCLC, and philosophers who have a strong interest in the methodological foundations of visualization research. The challenges we describe concern aspects of one single epistemic need: that of methodologically securing (an increase in) trust in visualizations. We discuss the lack of ground truths in the (digital) humanities and argue that trust in visualizations requires that we evaluate visualizations on the basis of ground truths that humanities scholars themselves create. We further argue that trust in visualiza- tions requires that a visualization provides provable guarantees on the faithfulness of the visual representation and that we must clearly communicate to the users which part of the visualization can be trusted and how much. Finally, we discuss transparency and accessi- bility in visualization research and provide measures for securing transparency and accessibility.


### Wheeler, Gregory (2016). Machine Epistemology and Big Data. In Lee McIntyre & Alex Rosenburg (eds.), _Routledge Companion to Philosophy of Social Science_. Routledge.

@incollection{Wheeler2016-WHEMEA,
  author = {Gregory Wheeler},
  year = {2016},
  booktitle = {Routledge Companion to Philosophy of Social Science},
  publisher = {Routledge},
  title = {Machine Epistemology and Big Data},
  editor = {Lee McIntyre and Alex Rosenburg}
}

>  In the age of big data and a machine epistemology that can anticipate, predict, and intervene on events in our lives, the problem once again is that a few individuals possess the knowledge of how to regulate these activities. But the question we face now is not how to share such knowledge more widely, but rather of how to enjoy the public beneﬁts bestowed by this knowledge without freely sharing it. It is not merely personal privacy that is at stake but a range of unsung beneﬁts that come from ignorance and forgetting, traits that are inherently human and integral to the functioning of our society. 

Machine Learning is a pragmatic approach to epistemology, one in which the goal or problem determins what features to extract from the data.

"Statistics confronts two questions. The first asks what can be inferred from data, given modeling assumptions that you choose, while the second asks after the reliability of those inferences." (p1)

"It is little surprise then that statistical methodology is viewed as a puzzle for traditional epistemology rather than a reservoir of answers." (p2)

"A machine can plow through combinatorial combinations of features involving enormous volumes of data, the scale of which is impossible for humans to replicate." (p6)

"Supervised learning problems are thus a form of enumerative induction" (p7)

"Instead, the goal of unsupervised learning is knowledge discovery" (p7)

"from the point of view of the philosophy of science, data science arguably does offer a new mode of inquiry insofar as we are now routinely handling population datasets directly or sample sizes so immense, such as in our NYC Taxi dataset, that they behave like population data." (p9)




### Wheeler, Gregory (2013). Models, Models, and Models. _Metaphilosophy_ 44 (3):293-300.

@article{Wheeler2013-WHEMMA,
  publisher = {Wiley-Blackwell},
  number = {3},
  pages = {293--300},
  author = {Gregory Wheeler},
  journal = {Metaphilosophy},
  volume = {44},
  title = {Models, Models, and Models},
  year = {2013}
}
  
> Michael Dummett famously maintained that analytic philosophy was simply philosophy that followed Frege in treating the philosophy of language as the basis for all other philosophy (1978, 441). But one important insight to emerge from computer science is how difficult it is to animate the linguistic artifacts that the analysis of thought produces. Yet, modeling the effects of thought requires a new skill that goes beyond analysis: procedural literacy. Some of the most promising research in philosophy makes use of a variety of modeling techniques that go beyond basic logic and elementary probability theory. What unifies this approach is a focus on what Alan Perlis called procedural literacy. This essay argues that the future spoils in philosophical research will disproportionally go to those who are procedurally literate. 

Procedural literacy — the ability to read and write processes.

"Just as an understanding of sound mathematical reasoning is taken to come only through understanding the meaning of the premises of an argument, for which an axiomatized system can be used to work out what follows from what, an understanding of sound reasoning is viewed as depending on a clear understanding of the meaning of the statements put forward in demonstratively valid argumentation. .... Yet, unlike in pure math- ematics, testing a philosophical thesis invariably involves a judgment of fit between the logical mechanics of the proposal and an intuitive feel for how the target phenomenon truly behaves. " (p294)

"Rather than logic, or even a wholesale switch to probability, my advice to young philosophers is for them to develop a dexterity with both formal and empirical methods. The shift I am proposing, from mere logical analysis to modeling of phenomena, requires procedural literacy." (p295)

"Programming is simply a means to express ideas about such procedures, and analytic philosophy is full of descriptions of procedures that are not well-defined." (p297)



