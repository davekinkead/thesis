# Lit Review


## Digital Philosophy

http://cdp.uwo.ca/


## Computational Philosophy to Date

@article{Andre2015-ANDSMA-4,
  journal = {Dissertatio},
  year = {2015},
  title = {Semantical Mutation, Algorithms and Programs},
  author = {Porto Andr\'e},
  pages = {44--76}
}

> The present article offers an explanation of perhaps Wittgenstein’s strangest and least intuitive thesis – the semantical mutation thesis – according to which one can never answer a mathematical conjecture because the new proof alters the very meanings of the terms involved in the original question. Instead of basing our justification on the distinction between mere calculation and proofs of isolated propositions, characteristic of Wittgenstein’s intermediary period, we generalize it to include conjectures involving effective procedures as well.

@porto2015 takes @martin-lof1984's [p29] ideas that lamba terms are programs to demonstrate the plausibility of Wittgenstein's _semantical mutation thesis_.  Code is used as an example of the _possible_.

> The  suggestion  is  thus  to  construe  the  rather  abstract-looking lambda-terms very  concretely,  as computer  programs. p54


---

@article{Bourget2010-BOUPPA,
  number = {4},
  author = {David Bourget},
  pages = {363--375},
  journal = {Social Epistemology},
  year = {2010},
  title = {Paperless Philosophy as a Philosophical Method},
  publisher = {Taylor \& Francis},
  volume = {24}
}

> I discuss the prospects for new forms of professional communication in philosophy. I argue that online discussions and online surveys ought to play a more important role in commu- nications between philosophers than they play today. However, there are major obstacles to the widespread adoption of these media as channels of communication between academics. I offer an overview of these obstacles and sketch a strategy for surmounting them. The strat- egy I propose involves the development of a new kind of service which could expand the reach of the analytic method in philosophy.

@bourget2010 looks at how digital methods can advance philosophy.

*Online Spaces*

Sub paper length arguments & refutations are currently lost thanks to the current publication mediums [p364] but a lack of prestege in blogs is limiting their use [p365]

> many verbally brief points which are potentially just as valuable as whole papers remain largely uncommunicated to the profession as a whole. [p364]

> I would not be surprised if it turned out that we spend significantly more time and energy on communication difficulties than philosophical problems per se when preparing articles [p364]

@bourget2010 also looks at online surveys and _philosophical registries_ that combine the two [p371]

> It is noteworthy that a registry of the kind sketched here could be particularly well suited to the application of a certain analytic methodology to philosophical problems. In some circles, it is widely believed that philosophical disputes are fuelled in large part by linguistic confusion—either the participants have a poor grasp of the meanings of the words they use, or they have a poor grasp of others’ meanings. On this view, the primary task of the philosopher is to dissipate the fog of language around the ideas under dispute. For most philosophical disputes, the closer we come to accomplishing this, the closer we will come to consensus. [p375]

---


# Argumentum in Silico

Thesis: That simulation as argument is a new and useful method of philosophy

1. position -- we've seen how a lot of philosophy involves modelling & simulation is just modelling. (1000)

  - The method of philosophy is a diverse one and it is hard to pin down - methodological pluralism
  - we use conceptual analysis to understand concepts, their meaning and relationships -- we make distinctions.
  - 
  - we use argument to explain and justify -- argument is logical model

2. motivate -- simulation also has the advantage of solving specific problems -- complexity (2000)

3. motivate -- simulation has its own problems however -- reproducabiity & realism (2000)

4. solve -- simulation as argument solves these problems

5. demonstrate??





> quote about simulation

In the the last chapter, we saw how computer simulation is situated within the diverse set of methodologies found within philosophy.  Computer simulations share many likenesses with other pre-digital methods such as conceptual analysis, theorising, and modelling.  Computer simulation can be considered as the digital analog to philosophy's most recognisable method - the thought experiment. 

In this chapter, I extend the function of computer simulation within the methodological framework of philosophy.  Specifically, I advance a new philosophical methodology that rather than simply using computers as an adjunct to analysis and argument, uses simulation as the argument and analysis itself.  

We might call this method _digital philosophy_ -- which can be broadly construed to include any way of doing philosophy with computers.  This by itself is nothing new.  Philosophers have been using computers to inform their work for over 30 years now.  Famous examples include @axelrod1984, @skyrms1996, and @list.

What set's my proposal apart from other forms of digital philosophy is the role simulation plays in the method. Digital philosophy to date has used simulation as a suppliment/anolog to the main work of writen argument.  Call this _argumentum cum silico_. I want to full integrate simulation into philosophy so that the simulation _is_ the argument itself.  Call this approach _argumentum in silico_ or _computational philosophy_.


## Simulation in Philosphy 

** how have digital methods been used in philosophy **

  - Digital Humanities
  - wikis & open access (value: promotion & disemination)
  - indexing & search (value: speed of research & productivity)
  - text analysis (value: volume of research, new insights)
  - network analysis (six degrees of francis bacon) (value: new insights, visual understanding)

** simulating complex systems **

  - there are distinct epsitemic challenges in areas like political philosophy
    - analytic intractability
    - complexity
    - emergence

  - axelrod

  - skryms 
    - @skyrms1996, 
    - @skyrms2004, 
    - @skyrms2010, 
    - Co-evolutionary Dynamics of Collective Action with Signaling for a Quorum
    - Inventing New Signals

  - list

## Simulation as Argument



  - cum vs in: what is the difference?
  - how would this work?
  - (knuth literate programming)

  - how does this address the problems of simulation?
    - reproducability
    - simulates theory, not reality

  - how does it do more than just that?
    - makes assumptions explicity (relate to qualities of philosophy)
    - persuasiveness

  - what is unique about this (what are the constraints)
    - premises of key arguments are formalised in code
    - simulation _is_ the argument
    - proof by demonstration, in areas where proof is thought impossible


## The Complexity Problem

  - many issues in philosophy are complex and can't be solved with analytic or empiric methods
  - simulation can offer a solution
  - monte carlo as a method, analytic vs statistically approximate answers

## The Problem with Simulations

Simulation faces similar problems to science 

  - replication of experiment
  - replication of results
    - system differences
    - floating point numbers

But it also faces a more specific problem (similar to thought experiments)
  
  - realism: does the simulation have high enough fidelity with reality to make warranted inferences from simulation results to reality.


    Verification:
    Determine whether the sim correctly
    implements the theory being investigated.
    Design verification, debugging, consistency
    checks

    Validation (Confirmation):
    Determine whether the
    sim as implemented conforms to the target
    Set initial conditions; run the sim; generate novel
    predictions; compare with reality


    There are many places where this confirmation process
    can break down. E.g.,
    Debugging
    Wrong initial conditions
    Wrong theory implemented
    Inaccessible predictions: if we can only retrodict, can
    we trust our sim?
    The first three lead to disconfirmation; the last to distrust

    There are analogues to each of these failures in real-world
    experimentation:
    Debugging/wrong initial conditions: Lead to incorrect
    prediction
    Wrong theory: Leads to incorrect prediction
    Inaccessible predictions: if we can only retrodict can
    we trust our theory?
    Not if our theory is manufactured to fit the existing
    data



## The Problem of Persuasion

  - how effective is argument at pursuading?
  - counter intuitive results are often hard to accept



---





Outline

## Simulation is philosophy too

** philosophy is mainly modelling **
  - what is a model
  - review how modelling is used in philosophy
  - what makes modelling philsophical?

** simulation is modelling **
  - non-computer simulations, though experiments?
  - are there types of simulation that aren't models?

** What do simulations do? **
  - create knowledge?
  - act as arguments?
  - provide evidence?


## An Example

  - Schelling's models of segregation
  - explain why it works


  - just as philosophy is hollow without thought experiments, so too is it hollow without computational approaches
  - computational approaches have much to offer philosophy because they overcome the epistemic limitations associated with complexity, intractability, and emergence.



- given certain assumptions, calculate the likelihood of particular outcomes.




---


## Simulation

  - what is simulation
  - how is it used in philosophy
  - how does it differ from modelling
  - what is the relationship between simulation and thought experiements



# Simulation and Knowledge (What problems does simulation solve?)


- segway?
- what is it?
- how is it like TE?
- what problems does simulation solve?

The term simulation however, .....  define it

  - physical vs mental
  - EG Electronic Numerical Integrator And Computer
  - simulation vs model
  - the role of time?
  - role of intervention
  - real vs hypotethical
  - explore phenomena vs communicate knowledge

> Paul Humphreys: “any computer-implemented method for exploring the properties of mathematical models where analytic methods are not available” (1991, 500).

> In its narrowest sense, a computer simulation is a program that is run on a computer and that uses step-by-step methods to explore the approximate behavior of a mathematical model. Usually this is a model of a real-world system (although the system in question might be an imaginary or hypothetical one). @winsberg2013

> More broadly, we can think of computer simulation as a comprehensive method for studying systems. 

> Winsberg 2003 (111). “Successful simulation studies do more than compute numbers. They make use of a variety of techniques to draw inferences from these numbers. Simulations make creative use of calculational techniques that can only be motivated extra-mathematically and extra-theoretically. As such, unlike simple computations that can be carried out on a computer, the results of simulations are not automatically reliable. Much effort and expertise goes into deciding which simulation results are reliable and which are not.”

> Another approach is to try to define “simulation” independently of the notion of computer simulation, and then to define “computer simulation” compositionally: as a simulation that is carried out by a programmed digital computer. On this approach, a simulation is any system that is believed, or hoped, to have dynamical behavior that is similar enough to some other system such that the former can be studied to learn about the latter.

> Simulation as an accurate approximation of some phenomena under investigation @me


> Two types of computer simulation are often distinguished: equation-based simulations and agent-based (or individual-based) simulations.

> Equation-based simulations are most commonly used in the physical sciences and other sciences where there is governing theory that can guide the construction of mathematical models based on differential equations

>  Agent-based simulations are similar to particle-based simulations in that they represent the behavior of n-many discrete individuals. But unlike equation-particle-based simulations, there are no global differential equations that govern the motions of the individuals. Rather, in agent-based simulations, the behavior of the individuals is dictated by their own local rules

 > Multiscale simulation models, in particular, couple together modeling elements from different scales of description.


Simulations needn't be restricted to the domain of computing however.
  - US engineers example
  - Kon Tiki


  - humphreys
  - winsberg
  - hartmann
  - Frigg and Reiss

- how are thought experiments and simulation related?


> the relationship between computer simulation and thought experiments has started to attract attention (see Behmel, 2001, pp. 98–108; Di Paolo et al., 2000; El Skaf and Imbert, 2013; Lenhard 2011; Stäudner, 1998).

> Accordingly, it has been argued that “computational modeling is largely replacing thought experimenting, and the latter will play only a limited role in future practice of science, especially in the sciences of complex nonlinear, dynamical phenomena” (see Chandrasekharan et al., 2012, p. 239).

> Maybe related to this is the proposal of Schulzke (2014) to think of video games philosophically as executable thought experiments.

  - TE are best thought of as a species of mental modelling.  mental simulation is a central aspect of cognition @nersessian1991 p431



###  what problems does simulation solve?


  - complexity: the model is very complex with many variables and interacting components
  - Emergence
  - analytically intractable problems, nonlinear relationships. problems without closed form expressions (In mathematics, a closed-form expression is a mathematical expression that can be evaluated in a finite number of operations.)
  - too many calculations for humans
  - invovles random variables or stochastic processes
  - communication in graphical or digital form

  - structural equations
  - experiment better by isolating variables and rerunning one-off events
  - emergence and discoverability
    - discovery vs justification

> Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. 

> (2011). Bedau argued that any conception of emergence must meet the twin hallmarks of explaining how the whole depends on its parts and how the whole is independent of its parts.

> Systems that produce emergent properties are mere mechanisms, but the mechanisms are very complex (they have very many independently interacting parts). As a result, there is no way to figure out exactly what will happen given a specific set of initial and boundary conditions, except to “crawl the causal web”.

- the limitations of simulations

  - claims about reality are only as good as a simulations fidelity to reality
  - address the "its not new knowledge" claim


- Models: 

> identify the features of these systems that were most sailent to to their investigations @weisberg2013 p4


Monte Carlo sim

 > MC simulations are computer algorithms that use randomness to calculate the properties of a mathematical model and where the randomness of the algorithm is not a feature of the target model.

> Grüne-Yanoff and Weirich (2010) offer the following reasoning: “The Monte Carlo approach does not have a mimetic purpose: It imitates the deterministic system not in order to serve as a surrogate that is investigated in its stead but only in order to offer an alternative computation of the deterministic system's properties” (p.30).

>  However, as Beisbart and Norton (2012, Other Internet Resources) point out, some MC simulations (viz. those that use MC techniques to solve stochastic dynamical equations referring to a physical system) are in fact simulations of the systems they study.

> There are three general categories of purposes to which computer simulations can be put. Simulations can be used for heuristic purposes, for the purpose of predicting data that we do not have, and for generating understanding of data that we do already have.


## Simulation in Philosophy


- how has simulation been used in other fields 
- why not so much in philosophy
- how simulation can help phil.





## Computational Philosophy as a Method

That's all well and good but how do we actually use simulation as a method what was the section I'm going to introduce an approach not calling computational philosophy or computational approaches to political philosophy.  I'm going to propose a new methodology computational approaches to political philosophy that overcomes the two biggest problems as I see them:

  1. replication and reproduction of simulation data used to justify arguments
  2. criticisms that simulations aren't accurate representations of reality

First problem of what location and reproduction is solved by, or at least mitigated by, a technique known as little of programming.  The Second solved by modelling theory rather than reality.  This approach and then in Abels all realises a third advantage – simulation is a form of communication.  In a sense literate programming of political theory allows to use simulation is a form of communication. It allows us to combine narrative argument experimentation and visualisation.

So what exactly is literate programming? Developed by the computer scientist Donald Knuth, literate programming is a way of writing software that results in documents of publishable quality which simultaniously argue for their own mathematical correctness. ((maybe fix this line)).  To program miserably one that explains the logic of software using natural language, interspersing that explanation with snippets of code.  I'm like traditional approaches to software development which typically focus on ordering on the order in source code for the computer compiler, literate programming 6 to order software the benefit the human reader.

According to Knuth:

> The practitioner of literate programming can be regarded as an essayist, whose main concern is with exposition and excellence of style. Such an author, with thesaurus in hand, chooses the names of variables carefully and explains what each variable means. He or she strives for a program that is comprehensible because its concepts have been introduced in an order that is best for human understanding, using a mixture of formal and informal methods that reinforce each other.  p ix  

In a sense "Programming is best regarded as the process of creating _works of literature_." @knuth p ix

> The practitioner of literate programming can be regarded as an essayist, whose main concern is with exposition and excellence of style. Such an author, with thesaurus in hand, chooses the names of variables carefully and explains what each variable means. He or she strives for a program that is comprehensible because its concepts have been introduced in an order that is best for human understanding, using a mixture of formal and informal methods that re ̈ınforce each other. p99

Augmented argumentation is a method allows us to combine literate programming as a method of software development with traditional philosophical argument.  Philosophy often uses formalisations to make particular claims explicit and precise.  Rather than use mathematical notation however, augmented argumentation uses computer source code as the formalisation.  This way the code that underpins simulation an intern underpins the arguments is fully explained two subject matter experts relax the requisite knowledge of code, while all the code necessary the running of situation is contained in the paper.  With literate programming, simulations and no longer black boxes this content is often difficult to obtain and check thanks to software licensing restrictions.

By making the code of simulations transparent we can overcome work with less and the likelihood the claims of philosophical argument supported by a faulty code. The code weather in the form of snippets within the argument for in the Annex of the paper is available for every reader to audit and run themselves.  This is especially important when modelling complex systems.  

The discipline of chaos theory - of deterministic systems sensitive to initial conditions - first came to white when (site the dude) discovered that tiny Errors in the rounding of floatingpoint numbers we can use weather simulations generated vastly different results.  (provide a short description of the history). 

When simulation remains blackbox available only under license or by request, the quality in The simulation code Jesus grounds for doubting the closing of the argument is supposed to support.  How do we know that the code is free of errors?  How can we trust that the code does what is claim to do?  Without access to source code, these questions cannot be answered satisfactorily.  Literate programming offers is the transparency background our confidence concerning the claims about the software even if the Individual reader lacks the skill to order the code themselves, the new fact that the code is provided with the arguments things that any possible errors a far more likely to be detected.

Little programming does introduce a new challenges however. The first which is the demands of different style an approach to software development.  It requires the code is encapsulated into smaller discrete functions maybe presented in the order benefiting the narrative of the argument rather than the order benefiting the compiler.  Is typically means all modular code is required makes heavy use the functions and macros.  Knuth describe this as "Instead of imagining that our main task is to instruct a _computer_ what to do, let us concentrate rather on explaining to _human beings_ what we want a computer to do. p99".

The other challenge is cultural rather than technical.... Code in papers!


#### The Stack 

The publication of scholarship occurs in a variety of mediums.  Historically this been printed journal all book, for the oral lecture.  The digitisation of scholarship percent publication move towards electronic formats - webpages, PDFs, video streaming, podcasts, or even MOOCs - massively open online courses.  Digitisation of scholarship has provided Turkey damages over traditional analogue methods.  The first is the use of distribution - Digital scholarship can be shared and downloaded instantly and at new zero cost.  The second is that scholarship can become more interactive.  Results can be presented graphically and in multiple dimensions.  Complex data can be visualised, queried, and explored.  Multimedia can be live streamed.



I'm going to combine literate programming using high-level languages with an almost natural language syntax with modern web technologies - coffeescript, javascript, and HTML.  Developed by Jeremy Ashkenas, coffee script is a "Little language that compiles to JavaScript".  HTML - HyperText Markup Language - is a declarative language that defines the content of webpages

It differs from JavaScript - 


- augmented argument
  - literate programming
  - why is this better?
    - simulation can be inteneded to communicate as well as investigate (@weisberg2013)
  - augmenting argument
    - beyond the paper


#### How it works

>  a program could be considered a publishable-quality document that argues mathematically for its own correctness. @random

> A different approach is that a program could be a document that teaches programming to the reader through its own example.  @random

So the definition of literate programming. Literate programming is the explanation of the program source code using natural language.  Little programming is the structure of software code so that it forms a narrative explanation of natural language.

Limit programming weeves together code and natural language explanation. 


> A literate program is an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code. Macros in a literate source file are simply title-like or explanatory phrases in a human language that describe human abstractions created while solving the programming problem, and hiding chunks of code or lower-level macros. These macros are similar to the algorithms in pseudocode typically used in teaching computer science. These arbitrary explanatory phrases become precise new operators, created on the fly by the programmer, forming a meta-language on top of the underlying programming language. @wikipedia

> A preprocessor is used to substitute arbitrary hierarchies, or rather "interconnected 'webs' of macros",[4] to produce the compilable source code with one command ("tangle"), and documentation with another ("weave"). The preprocessor also provides an ability to write out the content of the macros and to add to already created macros in any place in the text of the literate program source file, thereby disposing of the need to keep in mind the restrictions imposed by traditional programming languages or to interrupt the flow of thought. @wikipedia

Knuth Quotes

> Programming is best regarded as the process of creating _works of literature_.

> All of the major problems associated with computer programming - issues of reliability, portability learnability, maintainability, and efficiency - are ameliorated when programs and their dialogues with users become more literate.


> I believe that the time is ripe for significantly better documentation of programs, and that we can best achieve this by considering programs to be _works of literature_. p99


Each module in literate programming is introduced at the right psychological moment because the compiler orders the code. p236

Advantages

> literate programming provides higher-quality programs, since it forces programmers to explicitly state the thoughts behind the program, making poorly thought-out design decisions more obvious. @wikipedia

> The complexity was simply too daunting for my limited brain to handle; without literate programming, the whole enterprise would have flopped miserably. ... Literate programming is what you need to rise above the ordinary level of achievement.  @knuth interview with @binstock


---

