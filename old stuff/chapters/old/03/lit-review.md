# Lit Review - Philosophy & Computers


## Computational Philosophy










### Paperless Philosophy as a Philosophical Method

@article{Bourget2010-BOUPPA,
  journal = {Social Epistemology},
  title = {Paperless Philosophy as a Philosophical Method},
  publisher = {Taylor \& Francis},
  year = {2010},
  volume = {24},
  author = {David Bourget},
  number = {4},
  pages = {363--375}
}

> I discuss the prospects for new forms of professional communication in philosophy. I argue that online discussions and online surveys ought to play a more important role in communications between philosophers than they play today. However, there are major obstacles to the widespread adoption of these media as channels of communication between academics. I offer an overview of these obstacles and sketch a strategy for surmounting them. The strategy I propose involves the development of a new kind of service which could expand the reach of the analytic method in philosophy.

@bourget2010 looks at how digital methods can advance philosophy.

*Online Spaces*

Sub paper length arguments & refutations are currently lost thanks to the current publication mediums [p364] but a lack of prestege in blogs is limiting their use [p365]

> many verbally brief points which are potentially just as valuable as whole papers remain largely uncommunicated to the profession as a whole. [p364]

> I would not be surprised if it turned out that we spend significantly more time and energy on communication difficulties than philosophical problems per se when preparing articles [p364]

@bourget2010 also looks at online surveys and _philosophical registries_ that combine the two [p371]

> It is noteworthy that a registry of the kind sketched here could be particularly well suited to the application of a certain analytic methodology to philosophical problems. In some circles, it is widely believed that philosophical disputes are fuelled in large part by linguistic confusion—either the participants have a poor grasp of the meanings of the words they use, or they have a poor grasp of others’ meanings. On this view, the primary task of the philosopher is to dissipate the fog of language around the ideas under dispute. For most philosophical disputes, the closer we come to accomplishing this, the closer we will come to consensus. [p375]





  - Angius, Nicola & Tamburrini, Guglielmo (2011). Scientific Theories of Computational Systems  - in Model Checking. _Minds and Machines_ 21 (2):323-336.

  - Brey, Philip (2005). The epistemology and ontology of human-computer interaction. _Minds and   - Machines_ 15 (3-4):383-398.
  - Dodig Crnkovic, Gordana & Stuart, Susan (eds.) (2007). _Computation, Information, Cognition:   - The Nexus and the Liminal_. Cambridge Scholars Press.
  - Dodig-Crnkovic, Gordana (2007). WHERE DO NEW IDEAS COME FROM? HOW DO THEY EMERGE? -  - EPISTEMOLOGY AS COMPUTATION. In Christian Calude (ed.), _Randomness & Complexity, from   - Leibniz to Chaitin_.
  - Fuenmayor, David & Benzmueller, Christoph, A Case Study on Computational Hermeneutics: E. J. Lowe’s Modal Ontological Argument.
  - Gustafsson, Johan E. & Peterson, Martin (2012). A Computer Simulation of the Argument from   - Disagreement. _Synthese_ 184 (3):387-405.
  - Hellie, Benj (2017). David Lewis and the Kangaroo: Graphing philosophical progress. In   - Russell Blackford & Damien Broderick (eds.), _Philosophy's Future: The Problem of   - Philosophical Progress_. Oxford: Blackwell.
  - Leibovitz, David Pierre (2013). _A Unified Cognitive Model of Visual Filling-In Based on an  - Emergic Network Architecture_. Dissertation, Carleton University
  - Lokhorst, Gert-Jan (2011). Computational Meta-Ethics. _Minds and Machines_ 21 (2):261-274.
  - Luczak-Roesch, Markus ; Tinati, Ramine ; Aljaloud, Saud ; Hall, Wendy & Shadbolt, Nigel  - (2016). A universal socio-technical computing machine. In _International Conference on Web   - Engineering_.
  - Mizrahi, Moti (forthcoming). What Isn’t Obvious about ‘obvious’: A Data-driven Approach to   - Philosophy of Logic. In Andrew Aberdein & Matthew Inglis (eds.), _Advances in Experimental  - Philosophy of Logic and Mathematics_. London: Bloomsbury Press.
  - Müller, Vincent C. (2008). What a course on philosophy of computing is not. _APA Newsletter  - on Philosophy and Computers_ 8 (1):36-38.
  - Oppenheimer, Paul & Zalta, Edward N. (2011). A computationally-discovered simplification of  - the ontological argument. _Australasian Journal of Philosophy_ 89 (2):333 - 349.
  - Prakken, Henry (2012). Reconstructing Popov v. Hayashi in a framework for argumentation with   - structured arguments and Dungean semantics. _Artificial Intelligence and Law_ 20 (1):57-82.
  - Primiero, Giuseppe (2012). A contextual type theory with judgemental modalities for  - reasoning from open assumptions. _Logique and Analyse_ 220:579-600.
  - Rooij, Iris ; Wright, Cory & Wareham, Todd (2012). Intractability and the use of heuristics  - in psychological explanations. _Synthese_ 187 (2):471-487.
  - Sergeyev, Yaroslav (2013). Solving ordinary differential equations by working with   - infinitesimals numerically on the Infinity Computer. _Applied Mathematics and Computation_  - 219 (22):10668–10681.
  - Sergeyev, Yaroslav & Garro, Alfredo (2013). Single-tape and multi-tape Turing machines   - through the lens of the Grossone methodology. _Journal of Supercomputing_ 65 (2):645-663.
  - Sloman, Aaron (1992). The emperor's real mind -- Review of Roger Penrose's The Emperor's new   - Mind: Concerning Computers Minds and the Laws of Physics. _Artificial Intelligence_ 56  - (2-3):355-396.
  - Tamariz, Monica (2011). Could arbitrary imitation and pattern completion have bootstrapped   - human linguistic communication? _Interaction Studies_ 12 (1):36-62.
  - Vakarelov, Orlin (2012). The Information Medium. _Philosophy and Technology_ 25 (1):47-65.
  - van Deemter, Kees ; Gatt, Albert ; van Gompel, Roger P. G. & Krahmer, Emiel (2012). Toward a   - Computational Psycholinguistics of Reference Production. _Topics in Cognitive Science_ 4  - (2):166-183.
  - Varenne, Franck (2013). The Nature of Computational Things. In Frédéric Migayrou Brayer &  Marie-Ange (eds.), _Naturalizing Architecture_. Orléans: HYX Editions. pp. 96-105.
  - Wheeler, Billy (2017). Humeanism and Exceptions in the Fundamental Laws of Physics.  - _Principia: An International Journal of Epistemology_ 21 (3):317-337.






---

## Method












## Dissemination




## Computer Ontology





## Simulation

### Argument




### Reproducibility 







### Models

@article{Wheeler2013-WHEMMA,
  publisher = {Wiley-Blackwell},
  title = {Models, Models, and Models},
  volume = {44},
  number = {3},
  journal = {Metaphilosophy},
  year = {2013},
  author = {Gregory Wheeler},
  pages = {293--300}
}

> Michael Dummett famously maintained that analytic philosophy was simply philosophy that followed Frege in treating the philosophy of language as the basis for all other philosophy (1978, 441). But one important insight to emerge from computer science is how difficult it is to animate the linguistic artifacts that the analysis of thought produces. Yet, modeling the effects of thought requires a new skill that goes beyond analysis: procedural literacy. Some of the most promising research in philosophy makes use of a variety of modeling techniques that go beyond basic logic and elementary probability theory. What unifies this approach is a focus on what Alan Perlis called procedural literacy. This essay argues that the future spoils in philosophical research will disproportionally go to those who are procedurally literate. 

@article{David2005-DAVTLO-28,
  author = {Nuno David and Jaime Sichman and Helder Coleho},
  journal = {Journal of Artificial Societies and Social Simulation},
  volume = {8},
  number = {4},
  title = {The Logic of the Method of Agent-Based Simulation in the Social Sciences: Empirical and Intentional Adequacy of Computer Programs},
  year = {2005}
}

> The classical theory of computation does not represent an adequate model of reality for simulation in the social sciences. The aim of this paper is to construct a methodological perspective that is able to conciliate the formal and empirical logic of program verification in computer science, with the interpretative and multiparadigmatic logic of the social sciences. We attempt to evaluate whether social simulation implies an additional perspective about the way one can understand the concepts of program and computation. We demonstrate that the logic of social simulation implies at least two distinct types of program verifications that reflect an epistemological distinction in the kind of knowledge one can have about programs. Computer programs seem to possess a causal capability (Fetzer, 1999) and an intentional capability that scientific theories seem not to possess. This distinction is associated with two types of program verification, which we call empirical and intentional verification. We demonstrate, by this means, that computational phenomena are also intentional phenomena, and that such is particularly manifest in agent-based social simulation. Ascertaining the credibility of results in social simulation requires a focus on the identification of a new category of knowledge we can have about computer programs. This knowledge should be considered an outcome of an experimental exercise, albeit not empirical, acquired within a context of limited consensus. The perspective of intentional computation seems to be the only one possible to reflect the multiparadigmatic character of social science in terms of agent-based computational social science. We contribute, additionally, to the clarification of several questions that are found in the methodological perspectives of the discipline, such as the computational nature, the logic of program scalability, and the multiparadigmatic character of agent-based simulation in the social sciences. 


### Examples



### Philosophy of Computing

@article{Dodig-Crnkovic2011-DODSOM,
  number = {2},
  volume = {21},
  pages = {301--322},
  title = {Significance of Models of Computation, From Turing Model to Natural Computation},
  author = {Gordana Dodig{-}Crnkovic},
  journal = {Minds and Machines},
  year = {2011}
}

> The increased interactivity and connectivity of computational devices along with the spreading of computational tools and computational thinking across the fields, has changed our understanding of the nature of computing. In the course of this development computing models have been extended from the initial abstract symbol manipulating mechanisms of stand-alone, discrete sequential machines, to the models of natural computing in the physical world, generally concurrent asynchronous processes capable of modelling living systems, their informational structures and dynamics on both symbolic and sub-symbolic information processing levels. Present account of models of computation highlights several topics of importance for the development of new understanding of computing and its role: natural computation and the relationship between the model and physical implementation, interactivity as fundamental for computational modelling of concurrent information processing systems such as living organisms and their networks, and the new developments in logic needed to support this generalized framework. Computing understood as information processing is closely related to natural sciences; it helps us recognize connections between sciences, and provides a unified approach for modeling and simulating of both living and non-living systems 

@article{Boyer-Kassem2014-BOYLOM,
  author = {Thomas Boyer{-}Kassem},
  number = {4},
  year = {2014},
  journal = {International Studies in the Philosophy of Science},
  pages = {417--436},
  volume = {28},
  title = {Layers of Models in Computer Simulations}
}
   
> I discuss here the definition of computer simulations, and more specifically the views of Humphreys, who considers that an object is simulated when a computer provides a solution to a computational model, which in turn represents the object of interest. I argue that Humphreys's concepts are not able to analyse fully successfully a case of contemporary simulation in physics, which is more complex than the examples considered so far in the philosophical literature. I therefore modify Humphreys's definition of simulation. I allow for several successive layers of computational models, and I discuss the relations that exist between these models, the computer, and the object under study. An aim of my proposal is to clarify the distinction between computational models and numerical methods, and to better understand the representational and the computational functions of models in simulations. 
@article{Muller2008-MLLWAC-2,
  title = {What a Course on Philosophy of Computing is Not},
  number = {1},
  volume = {8},
  pages = {36--38},
  author = {Vincent C. M\"uller},
  journal = {APA Newsletter on Philosophy and Computers},
  year = {2008}
}

> Immanuel Kant famously defined philosophy to be about three questions: “What can I know? What should I do? What can I hope for?” (KrV, B833). I want to suggest that the three questions of our course on the philosophy of computing are: What is computing? What should we do with computing? What could computing do? 



@article{Evens2006-EVEOOO,
  volume = {11},
  title = {Object-Oriented Ontology, or Programming's Creative Fold},
  publisher = {Taylor & Francis},
  number = {1},
  journal = {Angelaki},
  pages = {89--97},
  author = {Aden Evens},
  year = {2006}
}

> This article asks what is creative about the act of programming. Observing that in most programming contexts, each line of code is written with a specific end in mind, it would seem as though there is little room for creativity, as the ends constrain the choices of means. However, there are many features of coding languages that open up creative possibilities. Object-oriented coding environments purport to make programming more about structures that humans might work with and less about features of computers, and other innovative coding techniques, such as RTTI design, suggest that there may be no limit to the creative possibilities of code. 


@incollection{Hernandez-EspinosaForthcoming-HERD-7,
  editor = {},
  publisher = {Springer Verlag},
  title = {Is There Any Real Substance to the Claims for a 'New Computationalism'?},
  year = {forthcoming},
  booktitle = {CiE Computability in Europe 2017},
  author = {Alberto Hernandez{-}Espinosa and Hernandez{-}Quiroz Francisco and Zenil Hector}
}

> 'Computationalism' is a relatively vague term used to describe attempts to apply Turing's model of computation to phenomena outside its original purview: in modelling the human mind, in physics, mathematics, etc. Early versions of computationalism faced strong objections from many (and varied) quarters, from philosophers to practitioners of the aforementioned disciplines. Here we will not address the fundamental question of whether computational models are appropriate for describing some or all of the wide range of processes that they have been applied to, but will focus instead on whether `renovated' versions of the \textit{new computationalism} shed any new light on or resolve previous tensions between proponents and skeptics. We find this, however, not to be the case, because the 'new computationalism' falls short by using limited versions of "traditional computation", or proposing computational models that easily fall within the scope of Turing's original model, or else proffering versions of hypercomputation with its many pitfalls. 


@book{Muller2016-MLLCAP-2,
  title = {Computing and Philosophy: Selected Papers From IACAP 2014},
  author = {Vincent C. M\"uller},
  year = {2016},
  publisher = {Springer}
}

> This volume offers very selected papers from the 2014 conference of the “International Association for Computing and Philosophy” (IACAP) - a conference tradition of 28 years. - - - Table of Contents - 0 Vincent C. Müller: - Editorial - 1) Philosophy of computing - 1 Çem Bozsahin: - What is a computational constraint? - 2 Joe Dewhurst: - Computing Mechanisms and Autopoietic Systems - 3 Vincenzo Fano, Pierluigi Graziani, Roberto Macrelli and Gino Tarozzi: - Are Gandy Machines really local? - 4 Doukas Kapantais: - A refutation of the Church-Turing thesis according to some interpretation of what the thesis says - 5 Paul Schweizer: - In What Sense Does the Brain Compute? - 2) Philosophy of computer science & discovery - 6 Mark Addis, Peter Sozou, Peter C R Lane and Fernand Gobet: - Computational Scientific Discovery and Cognitive Science Theories - 7 Nicola Angius and Petros Stefaneas: - Discovering Empirical Theories of Modular Software Systems. An Algebraic Approach. - 8 Selmer Bringsjord, John Licato, Daniel Arista, Naveen Sundar Govindarajulu and Paul Bello: - Introducing the Doxastically Centered Approach to Formalizing Relevance Bonds in Conditionals - 9 Orly Stettiner: - From Silico to Vitro: - Computational Models of Complex Biological Systems Reveal Real-world Emergent Phenomena - 3) Philosophy of cognition & intelligence - 10 Douglas Campbell: - Why We Shouldn’t Reason Classically, and the Implications for Artificial Intelligence - 11 Stefano Franchi: - Cognition as Higher Order Regulation - 12 Marcello Guarini: - Eliminativisms, Languages of Thought, & the Philosophy of Computational Cognitive Modeling - 13 Marcin Miłkowski: - A Mechanistic Account of Computational Explanation in Cognitive Science and Computational Neuroscience - 14 Alex Tillas: - Internal supervision & clustering: - A new lesson from ‘old’ findings? - 4) Computing & society - 15 Vasileios Galanos: - Floridi/Flusser: - Parallel Lives in Hyper/Posthistory - 16 Paul Bello: - Machine Ethics and Modal Psychology - 17 Marty J. Wolf and Nir Fresco: - My Liver Is Broken, Can You Print Me a New One? - 18 Marty J. Wolf, Frances Grodzinsky and Keith W. Miller: - Robots, Ethics and Software – FOSS vs. Proprietary Licenses 

@article{Dodig-Crnkovic2008-DODKGA,
  number = {2},
  year = {2008},
  volume = {6},
  author = {Gordana Dodig{-}Crnkovic},
  journal = {Journal of Systemics, Cybernetics and Informatics},
  title = {Knowledge Generation as Natural Computation}
}

> Knowledge generation can be naturalized by adopting computational model of cognition and evolutionary approach. In this framework knowledge is seen as a result of the structuring of input data (data → information → knowledge) by an interactive computational process going on in the agent during the adaptive interplay with the environment, which clearly presents developmental advantage by increasing agent’s ability to cope with the situation dynamics. This paper addresses the mechanism of knowledge generation, a process that may be modeled as natural computation in order to be better understood and improved. 




@incollection{Dodig-Crnkovic2007-DODP-3,
  author = {Gordana Dodig{-}Crnkovic},
  booktitle = {Randomness \& Complexity, from Leibniz to Chaitin},
  title = {WHERE DO NEW IDEAS COME FROM? HOW DO THEY EMERGE? - EPISTEMOLOGY AS COMPUTATION},
  year = {2007},
  editor = {Christian Calude}
}

> This essay presents arguments for the claim that in the best of all possible worlds (Leibniz) there are sources of unpredictability and creativity for us humans, even given a pancomputational stance. A suggested answer to Chaitin’s questions: “Where do new mathematical and biological ideas come from? How do they emerge?” is that they come from the world and emerge from basic physical (computational) laws. For humans as a tiny subset of the universe, a part of the new ideas comes as the result of the re-configuration and reshaping of already existing elements and another part comes from the outside as a consequence of openness and interactivity of the system. For the universe at large it is randomness that is the source of unpredictability on the fundamental level. In order to be able to completely predict the Universe-computer we would need the Universe-computer itself to compute its next state; as Chaitin already demonstrated there are incompressible truths which means truths that cannot be computed by any other computer but the universe itself. 

@article{Eden2007-EDETPO,
  number = {2},
  author = {Amnon H. Eden},
  title = {Three Paradigms of Computer Science},
  volume = {17},
  pages = {135--167},
  year = {2007},
  journal = {Minds and Machines},
  publisher = {Springer}
}

> We examine the philosophical disputes among computer scientists concerning methodological, ontological, and epistemological questions: Is computer science a branch of mathematics, an engineering discipline, or a natural science? Should knowledge about the behaviour of programs proceed deductively or empirically? Are computer programs on a par with mathematical objects, with mere data, or with mental processes? We conclude that distinct positions taken in regard to these questions emanate from distinct sets of received beliefs or paradigms within the discipline: – The rationalist paradigm, which was common among theoretical computer scientists, defines computer science as a branch of mathematics, treats programs on a par with mathematical objects, and seeks certain, a priori knowledge about their ‘correctness’ by means of deductive reasoning. – The technocratic paradigm, promulgated mainly by software engineers and has come to dominate much of the discipline, defines computer science as an engineering discipline, treats programs as mere data, and seeks probable, a posteriori knowledge about their reliability empirically using testing suites. – The scientific paradigm, prevalent in the branches of artificial intelligence, defines computer science as a natural (empirical) science, takes programs to be entities on a par with mental processes, and seeks a priori and a posteriori knowledge about them by combining formal deduction and scientific experimentation. We demonstrate evidence corroborating the tenets of the scientific paradigm, in particular the claim that program-processes are on a par with mental processes. We conclude with a discussion in the influence that the technocratic paradigm has been having over computer science. 

@article{Iwanska1993-IWALRI,
  journal = {Minds and Machines},
  volume = {3},
  author = {Lucja Iwaska},
  pages = {475--510},
  number = {4},
  title = {Logical Reasoning in Natural Language: It is All About Knowledge},
  year = {1993}
}

> A formal, computational, semantically clean representation of natural language is presented. This representation captures the fact that logical inferences in natural language crucially depend on the semantic relation of entailment between sentential constituents such as determiner, noun, adjective, adverb, preposition, and verb phrases.The representation parallels natural language in that it accounts for human intuition about entailment of sentences, it preserves its structure, it reflects the semantics of different syntactic categories, it simulates conjunction, disjunction, and negation in natural language by computable operations with provable mathematical properties, and it allows one to represent coordination on different syntactic levels. 

@article{Rapaport1999-RAPIIS,
  year = {1999},
  volume = {82},
  title = {Implementation Is Semantic Interpretation},
  pages = {109--130},
  number = {1},
  publisher = {The Hegeler Institute},
  author = {William J. Rapaport},
  journal = {The Monist}
}

> What is the computational notion of "implementation"? It is not individuation, instantiation, reduction, or supervenience. It is, I suggest, semantic interpretation. The online version differs from the published version in being a bit longer and going into a bit more detail. 






---

@unpublished{WadlerManuscript-WADPAP,
  author = {Philip Wadler},
  title = {Proofs Are Programs: 19th Century Logic and 21st Century Computing}
}

> As the 19th century drew to a close, logicians formalized an ideal notion of proof. They were driven by nothing other than an abiding interest in truth, and their proofs were as ethereal as the mind of God. Yet within decades these mathematical abstractions were realized by the hand of man, in the digital stored-program computer. How it came to be recognized that proofs and programs are the same thing is a story that spans a century, a chase with as many twists and turns as a thriller. At the end of the story is a new principle for designing programming languages that will guide computers into the 21st century. For my money, Gentzen’s natural deduction and Church’s lambda calculus are on a par with Einstein’s relativity and Dirac’s quantum physics for elegance and insight. And the maths are a lot simpler. I want to show you the essence of these ideas. I’ll need a few symbols, but not too many, and I’ll explain as I go along. To simplify, I’ll present the story as we understand it now, with some asides to fill in the history. First, I’ll introduce Gentzen’s natural deduction, a formalism for proofs. Next, I’ll introduce Church’s lambda calculus, a formalism for programs. Then I’ll explain why proofs and programs are really the same thing, and how simplifying a proof corresponds to executing a program. Finally, I’ll conclude with a look at how these principles are being applied to design a new generation of programming languages, particularly mobile code for the Internet. 